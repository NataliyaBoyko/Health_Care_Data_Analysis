{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "# Forecasting of deaths from Heart Failure on medical measurement\n",
    "\n",
    "# Lab 4. Model Evaluation and Refinement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is dedicated to the study of machine learning classification methods. The goal is to determine the impact of different measurements on mortality rate of patients with heart failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem to be solved in this lab is the classification of customers and analysis of the impact of various marketing campaigns on them.\n",
    "\n",
    "The peculiarities of classification analysis are that you must first correctly prepare a data set. In addition, today there are many different methods of classification. Each of them has its own characteristics and opportunities for analysis. An various classifiers are demonstrated in this lab, as well as join them with into ensemble. It is also demonstrated how all stages of training preparation and analysis can be combined with Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to download and pre-prepare data, classify and combine classifiers into an ensemble.\n",
    "This lab consists of the following steps:\n",
    "* Download data - download and display data from a file\n",
    "* Preliminary data preparation - preliminary analysis of data structure, change of data structure and tables\n",
    "* Pipeline classification - classification and analysis by grouping stages\n",
    "    * Logistic regression - classification and analysis of accuracy and errors using logistic regression\n",
    "    * Over-sampling problem - solve the problem of uneven distribution of data\n",
    "    * Ensemble of classifiers - study various classifiers and methods of combining them into an ensemble\n",
    "    * Decision tree - shows how to visualize the decision tree and determine the importance of factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical data was obtained from the https://www.kaggle.com/datasets/jackdaoud/marketing-data. This DataSet released under CC0: Public Domain license that allow of copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* [Python](https://www.python.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01) - middle level\n",
    "* [Pandas](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01) - middle level \n",
    "* [Matplotlib](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01) - basic level\n",
    "* [SeaBorn](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01) - basic level\n",
    "* [Scikit-Learn](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01) - middle level \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download DataSet from * .csv files\n",
    "* Conduct basic data analysis\n",
    "* Calculate new and change column types\n",
    "* Divide the DataSet into training and test\n",
    "* Use different machine learning classification methods\n",
    "* Combine classifiers into ensemble\n",
    "* Calculate accuracy and analyze errors\n",
    "* Visualize the decision tree\n",
    "* Combine all stages of data analysis with Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries/Define Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running outside Skills Network Labs.** This notebook was tested within Skills Network Labs. Running in another environment should work as well, but is not guaranteed and may require different setup routine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries such as Pandas, MatplotLib, SeaBorn, Scikit-Learn, imbalanced-learn, python-graphviz should be installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c intel scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from a .csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries should be imported before you can begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler                         \n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's disable warnings by **[warnings.filterwarnings()](https://docs.python.org/3/library/warnings.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further specify the value of the precision parameter equal to 2 to display two decimal signs (instead of 6 as default) by and  **[pd.options.display](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to download the data file from the repository by **[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**.\n",
    "\n",
    "We will use the same DataSet like in previous lab. Therefore next some steps will be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05QLEN/clean_df_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our DataSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data investigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study DataSet. As you can see DataSet consist 368 rows × 47 columns. As you can see DataSet consist information of different types. We should be sure that python recognized data types in right way. To do this we shoul use **[pandas.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=info#pandas.DataFrame.info)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Click to see attribute information</b></summary>\n",
    "    \n",
    "Input features (column names):\n",
    "\n",
    "1. `Age Group` - patient age divided by groups (categorical)\n",
    "2. `Marital Status` - married or single (categorical)\n",
    "3. `Lifestyle` - does the patient have healthy lifestyle (boolean)\n",
    "4. `Sleep` - does the patient sleep enough?(boolean)\n",
    "5. `Category` paid or free treatment (categorical)\n",
    "6. `Depression` - does patient feel depressed? (boolean)\n",
    "7. `Hyperlipidemia` - an excess of lipids or fats in your blood (boolean)\n",
    "8. `Smoking` - does the patient smoke? (boolean)\n",
    "9. `Diabetes` - does the patient have diabetes? (binary)\n",
    "10. `HTN` - hypertension, also known as high blood pressure (boolean)\n",
    "11. `Allergies` - does the patient have allergies? (boolean)\n",
    "12. `BP` - blood pressure (float, normalized)\n",
    "13. `Thrombolysis` - uses medications or a minimally invasive procedure to break up blood clots and prevent new clots from forming (binary)\n",
    "14. `BGR` - blood glucose level (int)\n",
    "15. `CPK` - creatine phosphokinase level (int)\n",
    "16. `ESR` - erythrocyte sedimentation rate (int)\n",
    "17. `WBC` - white blood cells, also known as leukocytes (int)\n",
    "18. `RBC` - red blood cells, also known as erythrocytes (float)\n",
    "19. `Hemoglobin` - hemoglobin level (float)\n",
    "20. `MCH` - mean corpuscular hemoglobin or the average amount in each of red blood cells of a hemoglobin (float)\n",
    "21. `MCHC` - mean corpuscular hemoglobin concentration (float)\n",
    "22. `PlateletCount` - count of platelets or thrombocytes (int)\n",
    "23. `Lymphocyte` - share of lymphocytes in blood (float)\n",
    "24. `Monocyte` -  share of monocytes in blood (float)\n",
    "25. `Eosinophil` - count of eosinophils (int)\n",
    "26. `Others` - other diseases, that weren't mentioned (categorical)\n",
    "27. `Diagnosis` - what is the patient's diagnosis? (float)\n",
    "28. `Hypersensitivity` - does the patient have hypersensitivity? (boolean)\n",
    "29. `Chest pain type` - patient's chest pain stage (int)\n",
    "30. `Resting BP` - resting blood pressure (float)\n",
    "31. `Serum cholesterol` - amount of total cholesterol in their blood (float)\n",
    "32. `FBS` - fasting blood sugar > 120 mg/dl (binary)\n",
    "33. `Resting electrocardiographic` - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy) (int)\n",
    "34. `Max heart rate` - patient's maximum heart rate achieved (int)\n",
    "35. `Angina` - does the patient have exercise induced angina (binary)\n",
    "36. `ST depression` - ST depression induced by exercise relative to rest (float)\n",
    "37. `Slope` - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) (int)\n",
    "38. `Vessels num` - number of major vessels (0-3) colored by flourosopy (int)\n",
    "39. `Thal` - 3 = normal; 6 = fixed defect; 7 = reversable defect (int)\n",
    "40. `Num` -  diagnosis of heart disease (angiographic disease status) (int)\n",
    "41. `Streptokinase` - used to dissolve blood clots that have formed in the blood vessels. Does the patient take it? (binary)\n",
    "42. `SK React` - what is the reaction from streptokinase (categorical)\n",
    "43. `Follow up` - number of patient's visiting time (int)\n",
    "44. `Max heart rate-binned` - patient's maximum heart rate achieved - binned (from Lab2) (categorical)\n",
    "45. `Gender-male` - is the patient male (from Lab2)? (binary)\n",
    "46. `Locality-urban` - is the patient's locality urban (from Lab2)? (binary)\n",
    "\n",
    "Output feature (desired target):\n",
    "\n",
    "47. `Mortality` - did the patient die of heart failure? (binary)\n",
    "    \n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipiline Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classification, the dataset must be divided into input and target factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns = ['Mortality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the input data set consists from 47 columns.\n",
    "\n",
    "As you can see, 7 columns are objects, and all others are numerical and boolean. To make classification, all numerical fields must be normalized and categorical fields must be digitized. This can be automated using the **[sklearn.preprocessing.OrdinalEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)** and **[sklearn. preprocessing.StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**.\n",
    "\n",
    "Since the machine learning process consists of several steps, each of which has the function `fit`,` predict` and etc, we can combine all these stages into one block using `Pipeline` (**[sklearn.pipeline.make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**), **[sklearn.compose.make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)** and visualize it with: **[sklearn.set_config()](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cat = list(x.select_dtypes(include=['object']).columns)\n",
    "col_num = list(x.select_dtypes(include=['float', 'int', 'bool']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = make_column_transformer((OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),col_cat),\n",
    "                                (StandardScaler(),col_num),\n",
    "                                remainder = 'passthrough')\n",
    "set_config(display = 'diagram')\n",
    "trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must separate DataSets for train and test DataSets for calculate accuracy of models. To do this we can use **[sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**. Let's separate DataSets in 0.33 proportion train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe let's create a logistic regression model (**[sklearn.linear_model.LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**) and add it to our `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "pipe_lr = make_pipeline(trans ,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model and calculate its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcross = cross_val_score(pipe_lr, x, y, cv=4)\n",
    "print(Rcross)\n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `cros_val_predict` to generate cross-validated estimates for each input data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = cross_val_predict(pipe_lr, x, y,cv=4)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "Let's calculate accuracy of this pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = pipe_lr.score(x_train, y_train)\n",
    "scores_test = pipe_lr.score(x_test, y_test)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the correctness of the classification with: **[sklearn.metrics.plot_confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)** and convince of these conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pipe_lr, x_test, y_test, cmap=plt.cm.Blues_r)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the table, our model predicts patient's mortality very well. At the same time, errors in the classification of patients are also not too big. The correct forecast is 101 patients. In 8 cases when the patient actually died of heart failure, the model shows that the patient will survive. Conversely, in the 2 cases, our model predicts that the patient will die of heart failure, but in fact he will survive. That is, the error is great for those patient who really has a risk to die.\n",
    "\n",
    "The `Recall` metric is used to assess the accuracy of only purchased goods: **[sklearn.metrics.recall_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = recall_score(y_train, pipe_lr.predict(x_train))\n",
    "scores_test = recall_score(y_test, pipe_lr.predict(x_test))\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from this metric, the accuracy is lower. Moreover, the accuracy of the training and test data are approximately the same. This means that in order to increase this metric of accuracy, it is necessary to increase the training sample. Let's analyze it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-sampling problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze our target column `Mortality` (**[seaborn.countplot()](https://seaborn.pydata.org/generated/seaborn.countplot.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of cases, when the patients survive is much greater than the number of deaths. To balance the data set, we can use a special function: **[imblearn.over_sampling.RandomOverSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROS = RandomOverSampler()\n",
    "o_x, o_y = ROS.fit_resample(x,y)\n",
    "sns.countplot(x = o_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we can see, these numbers are equal. Let's add this function to our `Pipeline`, fit the model and recalculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_s_lr = make_pipeline(trans, ROS, lr)\n",
    "pipe_s_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_s_lr.fit(x_train,y_train)\n",
    "scores_train = recall_score(y_train, pipe_s_lr.predict(x_train))\n",
    "scores_test = recall_score(y_test, pipe_s_lr.predict(x_test))\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, balancing the dataset has led to a sharp increase in the accuracy of the `Recall` metric.\n",
    "\n",
    "Let's analyze the errors of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pipe_s_lr, x_test, y_test, cmap=plt.cm.Blues_r)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the number of erroneous predictions about the patient, who will die, has decreased significantly. However, the error is high when the model predicts mortality rate of patients, who survived. The metric `Precision` is used to assess this accuracy.\n",
    "\n",
    "To further increase the `Recall` metric, it is necessary to change the model, because the accuracy of logistic regression on unknown data is about the same as on known data, and therefore it can no longer fit better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate `cross_val_score` and `cross_val_predict` for new pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcross = cross_val_score(pipe_s_lr, x, y, cv=4)\n",
    "print(Rcross)\n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n",
    "yhat = cross_val_predict(pipe_s_lr, x, y,cv=4)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test other classifiers and compare the results.\n",
    "We will test:\n",
    "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=logistic+regression#sklearn.linear_model.LogisticRegression)\n",
    "* [Linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=linear+svm#sklearn.svm.LinearSVR)\n",
    "* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n",
    "* [Extra Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)\n",
    "* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
    "* [Multi-layer Perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=mlpclassifier#sklearn.neural_network.MLPClassifier)\n",
    "* [Ada Boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01&highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n",
    "* [Gradient Boosting for classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)\n",
    "* [Bagging classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, different classifiers may err in different situations. Therefore, to compensate for each other's mistakes, it is necessary to use model ensembles by Voting Classifier.\n",
    "\n",
    "A **[Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05QLEN2531-2023-01-01)** is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n",
    "It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
    "\n",
    "Voting Classifier supports two types of votings.\n",
    "\n",
    "**Hard Voting**: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n",
    "\n",
    "\n",
    "**Soft Voting**: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Linear SVM\",\n",
    "         \"Decision Tree\", \"Extra Tree\", \"Random Forest\", \"Neural Net\",\n",
    "         \"AdaBoost\", \"GradientBoostingClassifier\", \"BaggingClassifier\", \"VotingClassifier\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    ExtraTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=0),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n",
    "    BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)]\n",
    "\n",
    "est = [(str(est), est) for est in classifiers]\n",
    "\n",
    "eclf = [VotingClassifier(\n",
    "     estimators=est,\n",
    "     voting='hard')]\n",
    "classifiers += eclf\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "scores_train_s = []\n",
    "scores_test_s = []\n",
    "\n",
    "for name, classif in zip(names, classifiers):\n",
    "    print(name,'fitting.....')\n",
    "    clf = make_pipeline(trans, classif)\n",
    "    clf.fit(x_train,y_train)\n",
    "    score_train = recall_score(y_train, clf.predict(x_train))\n",
    "    score_test = recall_score(y_test, clf.predict(x_test))    \n",
    "    scores_train.append(score_train)\n",
    "    scores_test.append(score_test)\n",
    "    \n",
    "    clf_s = make_pipeline(trans, ROS, classif)\n",
    "    clf_s.fit(x_train,y_train)\n",
    "    score_train_s = recall_score(y_train, clf_s.predict(x_train))\n",
    "    score_test_s = recall_score(y_test, clf_s.predict(x_test))    \n",
    "    scores_train_s.append(score_train_s)\n",
    "    scores_test_s.append(score_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the accuracy of classifiers for balanced and unbalanced data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index = names)\n",
    "res['Train'] = np.array(scores_train)\n",
    "res['Test'] = np.array(scores_test)\n",
    "res['Train Over Sampler'] = np.array(scores_train_s)\n",
    "res['Test Over Sampler'] = np.array(scores_test_s)\n",
    "\n",
    "res.index.name = \"Classifier accuracy\"\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[['Test', 'Test Over Sampler']].plot(kind=\"barh\", figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the balanced data set leads to a sharp increase in accuracy in all classifiers. It can also be seen that the most accurate models were GradientBoostingClassifier, AdaBoost and Neural Net. The ensemble of models showed perfect accuracy.\n",
    "\n",
    "Let's display the last classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we studied how to build training and test data sets and how to fit different classifiers, evaluate their accuracy and analyze errors.\n",
    "We also studied how to join them together in an ensemble and create a model based on Pipeline.\n",
    "We compared the accuracy of different classifiers and their ensemble and showed how they can be used in marketing on the example of customer classification.\n",
    "\n",
    "The accuracy of the decision was about 100%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\">Joseph Santarcangelo</a>\n",
    "\n",
    "### Other Contributors\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/mahdi-noorian-58219234/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\">Mahdi Noorian PhD</a>\n",
    "\n",
    "Bahare Talayian\n",
    "\n",
    "Eric Xiao\n",
    "\n",
    "Steven Dong\n",
    "\n",
    "Parizad\n",
    "\n",
    "Hima Vasudevan\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/fiorellawever/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\">Fiorella Wenver</a>\n",
    "\n",
    "<a href=\"https:// https://www.linkedin.com/in/yi-leng-yao-84451275/ \" target=\"_blank\" >Yi Yao</a>.\n",
    "\n",
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                 |\n",
    "| ----------------- | ------- | ---------- | ---------------------------------- |\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
