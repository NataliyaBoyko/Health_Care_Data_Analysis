{"cells":[{"cell_type":"markdown","id":"4df94568-4994-48d1-9660-d13cc0ef106e","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"\u003e\n","\u003c/center\u003e\n","\n","# Forecasting of deaths from Heart Failure on medical measurement\n","\n","# Lab 4. Model Development\n"]},{"cell_type":"markdown","id":"4f6b587d-cef0-4dce-9db2-1c2e10c45c51","metadata":{},"outputs":[],"source":["The purpose of this lab is to master classification patients for machine learning models.\n","\n","After completing this lab you will be able to:\n","\n","1. preprocess (normilize and transform categorical data) and create DataSet\n","2. features selection\n","3. make classification of clients\n","4. visualize decision tree of classification model  \n"]},{"cell_type":"markdown","id":"07b4207b-9fde-4829-97b8-b3f31460e2fd","metadata":{},"outputs":[],"source":["## Outline\n"]},{"cell_type":"markdown","id":"85c25ffd-738d-474f-8275-c62e80bc9d6e","metadata":{},"outputs":[],"source":["* Materials and Methods\n","* General Part\n","  * Import Libraries\n","  * Load the Dataset\n","  * Data preparation\n","      * Data transformation\n","      * Encoding and Normalization\n","  * Features selection\n","      * Chi-Squared Statistic\n","      * Mutual Information Statistic\n","      * Feature Importance\n","      * Correlation Matrix with Heatmap\n","  * Decision tree \n","      * Build model\n","      * Visualization of decision tree\n","  * Classification models\n","      * Extra Trees Classifier\n","      * Logistic regression \n","* Authors\n"]},{"cell_type":"markdown","id":"d699675a-a142-4ecf-b698-b6c07dcdebd9","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"3be7d1be-378d-44a4-a447-a693cb8d408b","metadata":{},"outputs":[],"source":["## Materials and Methods\n"]},{"cell_type":"markdown","id":"78f68dc1-afba-4385-ac02-2c9ca65fbe9d","metadata":{},"outputs":[],"source":["The data that we are going to use for this is a subset of an open source The Heart Failure Prediction Dataset. https://www.kaggle.com/datasets/asgharalikhan/mortality-rate-heart-patient-pakistan-hospital.\n","\n","\u003e This dataset is public available for research.\n","Please include this citation if you plan to use this database:\n","The data contains complete history of heart patients (so data scientists from different parts of the world can work with it). The dataset is collected from Pakistan, Faisalabad hospital named institute of cardiology\n","\n","In this lesson, we will try to give answers to a set of questions that may be relevant when analyzing heart failure data:\n","\n","1. What are the most useful Python libraries for classification analysis?\n","2. How to transform category data?\n","3. How to create DataSet?\n","4. How to do features selection?\n","5. How to make, fit and visualize classification model?\n","\n","In addition, we will make the conclusions for the obtained results of our classification analysis to predict mortality rate of patients with heart disease.\n"]},{"cell_type":"markdown","id":"ba3628b2-0f24-47b8-b2b9-cc8a78c3e2b6","metadata":{},"outputs":[],"source":["[Scikit-learn](https://scikit-learn.org/stable/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01) (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n"]},{"cell_type":"code","id":"4c4b6b4c-e6ec-45bc-958d-6e20fc767061","metadata":{},"outputs":[],"source":["!conda install --yes scikit-learn==0.24.2\n!conda install --yes python-graphviz"]},{"cell_type":"markdown","id":"5b39a44a-080f-481d-9b35-ba8357486a4c","metadata":{},"outputs":[],"source":["## Import Libraries\n"]},{"cell_type":"markdown","id":"753b9a20-50ef-49cd-a587-b13e53f7c69a","metadata":{},"outputs":[],"source":["Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"]},{"cell_type":"code","id":"1327c42a-4554-4e3c-b8b9-96a925c9ee10","metadata":{},"outputs":[],"source":["import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n# Data transformation\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n# Features Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n# Classificators\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\n# warnings deactivate\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz"]},{"cell_type":"markdown","id":"c7a4c91d-f306-4b45-b84f-3d0a3add50b3","metadata":{},"outputs":[],"source":["Further specify the value of the `precision` parameter equal to 2 to display two decimal signs (instead of 6 as default).\n"]},{"cell_type":"code","id":"e8e56679-52d7-4352-a42e-fc829f50c388","metadata":{},"outputs":[],"source":["pd.options.display.float_format = '{:.2f}'.format"]},{"cell_type":"markdown","id":"116b618e-0cb0-4155-a9a0-f88e7bbe78cf","metadata":{},"outputs":[],"source":["## Load the Dataset\n"]},{"cell_type":"markdown","id":"32c087b0-8a35-43fa-997d-77c6b9ec67cf","metadata":{},"outputs":[],"source":["We will use the same DataSet that we have saved in previous labs.\n"]},{"cell_type":"code","id":"73c954a0-6f41-4b99-af74-59deb20bf70a","metadata":{},"outputs":[],"source":["df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0MI1EN/clean_df_new.csv')\ndf.head(5)"]},{"cell_type":"code","id":"3cab67ed-1cea-40ef-99d3-7a0b5066a644","metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","id":"3375ab34-fcfd-434a-9c8a-982efd4ac47a","metadata":{},"outputs":[],"source":["As you can see DataSet consist 47 columns. Target column is 'Mortality'. Also DataSet consist 368 rows. In previous labs we investigated these columns.\n"]},{"cell_type":"markdown","id":"e5c6fd60-3f79-4de9-ba38-e13711be7176","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\n","\u003csummary\u003e\u003cb\u003eClick to see attribute information\u003c/b\u003e\u003c/summary\u003e\n","\n","Input features (column names):\n","\n","1. `Age Group` - patient age divided by groups (categorical)\n","2. `Marital Status` - married or single (categorical)\n","3. `Lifestyle` - does the patient have healthy lifestyle (boolean)\n","4. `Sleep` - does the patient sleep enough?(boolean)\n","5. `Category` paid or free treatment (categorical)\n","6. `Depression` - does patient feel depressed? (boolean)\n","7. `Hyperlipidemia` - an excess of lipids or fats in your blood (boolean)\n","8. `Smoking` - does the patient smoke? (boolean)\n","9. `Diabetes` - does the patient have diabetes? (binary) \n","10. `HTN` - hypertension, also known as high blood pressure (boolean) \n","11. `Allergies` - does the patient have allergies? (boolean)\n","12. `BP` - blood pressure (float, normalized) \n","13. `Thrombolysis` - uses medications or a minimally invasive procedure to break up blood clots and prevent new clots from forming (binary) \n","14. `BGR` - blood glucose level (int) \n","15. `CPK` - creatine phosphokinase level (int)\n","16. `ESR` - erythrocyte sedimentation rate (int) \n","17. `WBC` - white blood cells, also known as leukocytes (int) \n","18. `RBC` - red blood cells, also known as erythrocytes (float) \n","19. `Hemoglobin` - hemoglobin level (float) \n","20. `MCH` - mean corpuscular hemoglobin or the average amount in each of red blood cells of a hemoglobin (float)\n","21. `MCHC` - mean corpuscular hemoglobin concentration (float)\n","22. `PlateletCount` - count of platelets or thrombocytes (int)\n","23. `Lymphocyte` - share of lymphocytes in blood (float)\n","24. `Monocyte` -  share of monocytes in blood (float)\n","25. `Eosinophil` - count of eosinophils (int)\n","26. `Others` - other diseases, that weren't mentioned (categorical)\n","27. `Diagnosis` - what is the patient's diagnosis? (float)\n","28. `Hypersensitivity` - does the patient have hypersensitivity? (boolean)\n","29. `Chest pain type` - patient's chest pain stage (int)\n","30. `Resting BP` - resting blood pressure (float)\n","31. `Serum cholesterol` - amount of total cholesterol in their blood (float)\n","32. `FBS` - fasting blood sugar \u003e 120 mg/dl (binary)\n","33. `Resting electrocardiographic` - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy) (int)\n","34. `Max heart rate` - patient's maximum heart rate achieved (int)\n","35. `Angina` - does the patient have exercise induced angina (binary)\n","36. `ST depression` - ST depression induced by exercise relative to rest (float)\n","37. `Slope` - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) (int)\n","38. `Vessels num` - number of major vessels (0-3) colored by flourosopy (int)\n","39. `Thal` - 3 = normal; 6 = fixed defect; 7 = reversable defect (int)\n","40. `Num` -  diagnosis of heart disease (angiographic disease status) (int)\n","41. `Streptokinase` - used to dissolve blood clots that have formed in the blood vessels. Does the patient take it? (binary)\n","42. `SK React` - what is the reaction from streptokinase (categorical)\n","43. `Follow up` - number of patient's visiting time (int)\n","44. `Max heart rate-binned` - patient's maximum heart rate achieved - binned (from Lab2) (categorical)\n","45. `Gender-male` - is the patient male (from Lab2)? (binary)\n","46. `Locality-urban` - is the patient's locality urban (from Lab2)? (binary)\n","\n","Output feature (desired target):\n","\n","47. `Mortality` - did the patient die of heart failure? (binary)\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"dbbef939-2905-40d4-98d5-ae1c2ef666cb","metadata":{},"outputs":[],"source":["Our goal is to create the model for predicting mortality caused by Heart Failure. To do this we must analize and prepare data for such type of model.\n"]},{"cell_type":"markdown","id":"50bbc5c7-9001-414d-ac60-3460dcf65000","metadata":{},"outputs":[],"source":["## Data preparation\n"]},{"cell_type":"markdown","id":"7303ae3f-2941-4106-ac5d-5bb5d163d2fc","metadata":{},"outputs":[],"source":["### Data transformation\n"]},{"cell_type":"markdown","id":"a9359307-a9c0-4161-a59a-f497e928d38f","metadata":{},"outputs":[],"source":["First of all we should investigate how pandas recognized types of features\n"]},{"cell_type":"code","id":"c2907dfa-bc3a-4969-968a-e25a20efabcd","metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","id":"0a803f04-86eb-473c-af8d-e88ecd401973","metadata":{},"outputs":[],"source":["As you can see all categorical features was recogized like object. We must change thair type on \"categorical\". \n"]},{"cell_type":"code","id":"0acb861f-d0b6-44bc-89ed-d94cbd519c60","metadata":{},"outputs":[],"source":["col_cat = list(df.select_dtypes(include=['object']).columns)\ncol_cat"]},{"cell_type":"markdown","id":"d2f1e7aa-ab13-47f8-a93d-7de9d2a47f07","metadata":{},"outputs":[],"source":["Let's look at the dataset size.\n"]},{"cell_type":"code","id":"0a20f28f-013e-4ee8-858f-b350bd2389a4","metadata":{},"outputs":[],"source":["df.loc[:, col_cat] = df[col_cat].astype('category')\ndf.info()"]},{"cell_type":"markdown","id":"01d369fc-20b9-45df-af57-d86a7fe149c9","metadata":{},"outputs":[],"source":["To see the unical values of exact feature (column) we can use:\n"]},{"cell_type":"code","id":"89c10c9f-ab24-42e1-bcbe-2603a42f2b8b","metadata":{},"outputs":[],"source":["df['Age Group'].unique()"]},{"cell_type":"markdown","id":"d4d58f1a-1653-4549-8262-746ae4352d9d","metadata":{},"outputs":[],"source":["As was signed earlier the dataset contains 368 objects (rows), for each of which 47 features are set (columns), including 1 target feature (y). 7 features are categorical. These data type of values cannot use for classification. We must transform it to int or float.\n","To do this we can use **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)** and **[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)**. These functions can encode categorical features as an integer array.\n","\n","First of all we separate DataSet on input and output(target) DataSets\n"]},{"cell_type":"code","id":"ef60924f-543d-4644-8acf-d685baab3f5b","metadata":{},"outputs":[],"source":["X = df.drop(['Mortality'], axis=1)  #input columns\ny = df['Mortality']   #target column"]},{"cell_type":"markdown","id":"b1dc62df-dd5a-4360-b193-1a7d66750d2e","metadata":{},"outputs":[],"source":["### Encoding and Normalization\n"]},{"cell_type":"markdown","id":"c9333572-8ee3-419f-94b3-f5198d70dda2","metadata":{},"outputs":[],"source":["Than create list of categorical fields and transform thair values to int arrays: (Replace ##YOUR CODE GOES HERE## with your Python code.)\n"]},{"cell_type":"code","id":"ba6ed9be-cea3-4062-93a2-13f60f71e7cb","metadata":{},"outputs":[],"source":["col_cat = list(X.select_dtypes(include=['category']).columns)\noe = OrdinalEncoder()\noe.fit(X[col_cat])\nX_cat_enc = oe.transform(X[col_cat])"]},{"cell_type":"code","id":"cca59173-c627-4271-924f-2b594d0a0635","metadata":{},"outputs":[],"source":["X_cat_enc"]},{"cell_type":"markdown","id":"41a2f08e-7bec-4b3a-a238-1bfbba02abd1","metadata":{},"outputs":[],"source":["Than we must transform arrays back into DataFrame:\n"]},{"cell_type":"code","id":"689e89f9-a285-4683-8147-970b71bf9d25","metadata":{},"outputs":[],"source":["X_cat_enc = pd.DataFrame(X_cat_enc)\nX_cat_enc.columns = col_cat"]},{"cell_type":"markdown","id":"2e1bf089-547d-4f71-b6eb-c8b111a7087b","metadata":{},"outputs":[],"source":["Numerical fields can have different scale and can consists negative values. These will lead to round mistakes and exeptions for some AI methods. To avoid it these features must be normalized.\n","\n","Let's create list of numerical fields and normilize it using by **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)**\n"]},{"cell_type":"code","id":"9effa683-7c51-47d6-827e-7519c10c3a82","metadata":{},"outputs":[],"source":["col_num = list(X.select_dtypes(include=['float', 'int', 'bool']).columns)\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_num_enc = scaler.fit_transform(X[col_num])"]},{"cell_type":"code","id":"0baa8a56-9d2b-47e9-b79a-caca21ed41b1","metadata":{},"outputs":[],"source":["X_num_enc"]},{"cell_type":"markdown","id":"9fe758c1-357f-4db5-b400-a0f433b35d60","metadata":{},"outputs":[],"source":["Like in previous case transform back obtained arrays into DataFrame\n"]},{"cell_type":"code","id":"7d332ffe-70ba-4af2-9709-3d25d13b5d53","metadata":{},"outputs":[],"source":["X_num_enc = pd.DataFrame(X_num_enc)\nX_num_enc.columns = col_num\nX_num_enc"]},{"cell_type":"markdown","id":"c6aca17f-d4b4-4913-bbc1-3245a7998713","metadata":{},"outputs":[],"source":["Than we should concatenate these DataFrames in one input DataFrame\n"]},{"cell_type":"code","id":"c797f9ff-f7a2-461e-9562-027e37e91ea2","metadata":{},"outputs":[],"source":["x_enc = pd.concat([X_cat_enc, X_num_enc], axis=1)\nx_enc"]},{"cell_type":"markdown","id":"a20778ed-5aaa-4e11-be86-72704af89734","metadata":{},"outputs":[],"source":["Our target column is already normalized, so we don't need to encode it.\n"]},{"cell_type":"markdown","id":"9df1b9e1-cf4b-432d-8924-06d9e8d564b4","metadata":{},"outputs":[],"source":["## Features selection\n"]},{"cell_type":"markdown","id":"7034087a-a464-46d9-a135-629f4fe11347","metadata":{},"outputs":[],"source":["As was signed before input fields consists 46 features. Of course some of them are more significant for classification.\n","\n","There are two popular feature selection techniques that can be used for categorical input data and a categorical (class) target variable.\n","\n","They are:\n","\n","* Chi-Squared Statistic.\n","* Mutual Information Statistic.\n","\n","Let’s take a closer look at each in turn.\n","\n","To do this we can use **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)**\n"]},{"cell_type":"markdown","id":"78d0464c-bfaf-4f42-a0a6-e39341724f98","metadata":{},"outputs":[],"source":["### Chi-Squared Statistic\n"]},{"cell_type":"markdown","id":"4fdebbba-4ef5-4517-8328-ade990d55eaf","metadata":{},"outputs":[],"source":["Pearson’s chi-squared statistical hypothesis test is an example of a test for independence between categorical variables.\n","\n","You can learn more about this statistical test in the tutorial:\n","\n","[A Gentle Introduction to the Chi-Squared Test for Machine Learning](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)\n","The results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset.\n","\n","The scikit-learn machine library provides an implementation of the chi-squared test in the **[chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01#sklearn.feature_selection.chi2)** function. This function can be used in a feature selection strategy, such as selecting the top k most relevant features (largest values) via the SelectKBest class.\n","\n","For example, we can define the SelectKBest class to use the chi2() function and select all (or most significant) features.\n"]},{"cell_type":"markdown","id":"50d11e7d-786e-4786-81d9-5ea6bcfd61d9","metadata":{},"outputs":[],"source":["Apply SelectKBest class to extract top 10 best features\n"]},{"cell_type":"code","id":"fbfdfa98-0a9c-4d18-8894-de539bca13ac","metadata":{},"outputs":[],"source":["bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(x_enc, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)"]},{"cell_type":"markdown","id":"1b027091-c847-4595-a7d6-dab5483986b6","metadata":{},"outputs":[],"source":["concat two dataframes for better visualization \n"]},{"cell_type":"code","id":"801d7619-b798-4d91-ad09-314befc673e0","metadata":{},"outputs":[],"source":["featureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features"]},{"cell_type":"markdown","id":"92c68860-9563-4b89-bb1f-fb1b27d9e503","metadata":{},"outputs":[],"source":["### Mutual Information Statistic\n"]},{"cell_type":"markdown","id":"3dadf34c-fcd9-4556-9a45-c3c688417c0a","metadata":{},"outputs":[],"source":["Mutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection.\n","\n","Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n","\n","[You can learn more about mutual information in the following tutorial.](https://machinelearningmastery.com/information-gain-and-mutual-information?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)\n","\n","The scikit-learn machine learning library provides an implementation of mutual information for feature selection via the **[mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01#sklearn.feature_selection.mutual_info_classif)** function.\n","\n","Like chi2(), it can be used in the SelectKBest feature selection strategy (and other strategies).\n"]},{"cell_type":"code","id":"0ac8e126-b930-4c76-9e91-932ea333ea1c","metadata":{},"outputs":[],"source":["bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(x_enc, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features"]},{"cell_type":"markdown","id":"0a367f5c-6351-4a6e-b79b-66566e306456","metadata":{},"outputs":[],"source":["As you can see these 2 function select different significant features.\n"]},{"cell_type":"markdown","id":"7f0e2daa-059e-410e-949b-945109c77ba1","metadata":{},"outputs":[],"source":["### Feature Importance\n"]},{"cell_type":"markdown","id":"7563def7-b289-4cb5-a318-05b73a1eed1c","metadata":{},"outputs":[],"source":["You can get the feature importance of each feature of your DataFrame by using the feature importance property of the exact classification model.\n","Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n","For example:\n","Feature importance is an inbuilt class that comes with **[Tree Based Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)**, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.\n"]},{"cell_type":"markdown","id":"23389cce-6824-4c51-ad63-f0422e59c46b","metadata":{},"outputs":[],"source":["Let's create and fit the model:\n"]},{"cell_type":"code","id":"2272112c-9e57-4555-9405-b246ac86bfb1","metadata":{},"outputs":[],"source":["model = ExtraTreesClassifier()\nmodel.fit(x_enc, y)"]},{"cell_type":"markdown","id":"acf25970-0ade-4bdd-b081-f2aa54d31a45","metadata":{},"outputs":[],"source":["use inbuilt class feature_importances of tree based classifiers\n"]},{"cell_type":"code","id":"904c9daa-de31-4cce-9d99-696989b2c65d","metadata":{},"outputs":[],"source":["print(model.feature_importances_)"]},{"cell_type":"markdown","id":"367dfe81-e396-402b-b2c0-c921267d8170","metadata":{},"outputs":[],"source":["Let's transform it into Series and plot graph of feature importances for better visualization\n"]},{"cell_type":"code","id":"98e47b23-94b8-4fc7-a3a5-374b04cbb027","metadata":{},"outputs":[],"source":["feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"]},{"cell_type":"markdown","id":"59f0aa17-9a2c-4c5d-90bf-b37ec6d65a2a","metadata":{},"outputs":[],"source":["You can see that for Extra Tree Classifier impotance of features are different than in previous cases. It means that there are not exact rules for features selection. And their impotance strictly depedence on model.\n"]},{"cell_type":"markdown","id":"adefaeac-73d0-4a8a-8c6a-2ad3958690de","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \u003ch1\u003eQuestion 1:\u003c/h1\u003e\n","    \u003cp\u003ePlot graph of 5 least important features\u003c/p\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"98e2699b-adba-4a5b-90b3-d7d585b1f682","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n"]},{"cell_type":"markdown","id":"09d99780-53f9-4155-91d7-918d68025025","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","feat_importances.nsmallest(5).plot(kind='barh')\n","plt.show()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"b659eee7-0a12-40d2-b887-87730d3d6c9b","metadata":{},"outputs":[],"source":["### Correlation Matrix with Heatmap\n"]},{"cell_type":"markdown","id":"4b77950f-abcb-4f7c-9a47-0e1c0a26342e","metadata":{},"outputs":[],"source":["Correlation states how the features are related to each other.\n","Correlation can be positive (increase in one value of feature increases the value of the other variable) or negative (increase in one value of feature decreases the value of the other variable)\n","Heatmap makes it easy to identify which features are most related to the other variable, we will plot heatmap of correlated features using the seaborn library.\n"]},{"cell_type":"code","id":"c033d505-42d4-4b5a-b878-be2cf6fdfbd1","metadata":{},"outputs":[],"source":["corrmat = x_enc.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(35,35))\ng=sns.heatmap(x_enc[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"]},{"cell_type":"markdown","id":"71e6c89a-5b50-4b08-a9e7-1e6f4777d857","metadata":{},"outputs":[],"source":["We have already removed strictly correlated columns, thus we can use all these features.\n"]},{"cell_type":"markdown","id":"f4db95fb-557c-4002-bfd4-e2c99aa86a9c","metadata":{},"outputs":[],"source":["## Classification models\n"]},{"cell_type":"markdown","id":"7b539de5-bc23-4e19-954a-a98cdec71a3b","metadata":{},"outputs":[],"source":["## Decision tree\n"]},{"cell_type":"markdown","id":"e1bf0e4d-da82-4af3-9b92-137c10f52ed6","metadata":{},"outputs":[],"source":["### Build model\n"]},{"cell_type":"markdown","id":"3d690900-2851-474f-b639-dec56b2c11fa","metadata":{},"outputs":[],"source":["As shown, the previous methods have high accuracy. However, the biggest drawback is the inability to visualize or justify the decision.\n"]},{"cell_type":"markdown","id":"831b5d23-7ef0-44c8-b458-7bb0168a69d5","metadata":{},"outputs":[],"source":["Decision trees are a popular supervised learning method for a variety of reasons. Benefits of decision trees include that they can be used for both regression and classification, they don’t require feature scaling, and they are relatively easy to interpret as you can visualize decision trees. This is not only a powerful way to understand your model, but also to communicate how your model works. Consequently, it would help to know how to make a visualization based on your model.\n"]},{"cell_type":"markdown","id":"3967b351-1a5b-4f99-afa7-7a0485869087","metadata":{},"outputs":[],"source":["A **[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)** is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach to the leaf, the sample is propagated through nodes, starting at the root node. In each node a decision is made, to which descendant node it should go. A decision is made based on the selected sample’s feature. Decision Tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric.\n"]},{"cell_type":"markdown","id":"626404df-06b9-4cd3-88ac-71dc51f135b0","metadata":{},"outputs":[],"source":["This metod allows also to calculate features impotance.\n","Let's calculate them. Choice best 10 of them. Refit the model and visualize decision tree.\n"]},{"cell_type":"code","id":"4dab4a21-ba14-4dfc-a695-0c650acd04ca","metadata":{},"outputs":[],"source":["model = DecisionTreeClassifier()\nmodel.fit(x_enc, y)\nyhat = model.predict(x_enc)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"0a608ed5-bfcc-4d8c-ac44-aa4eb42fc2d6","metadata":{},"outputs":[],"source":["Create user function that will calculate accuracy of defined classificator model:\n"]},{"cell_type":"code","id":"eb3c79f8-e92b-48f0-9c06-9b8f0e95168c","metadata":{},"outputs":[],"source":["def model_ac(x, y, clf):\n    model.fit(x, y)\n    yhat = model.predict(x)\n    accuracy = accuracy_score(y, yhat)\n    return accuracy"]},{"cell_type":"markdown","id":"a6a46de2-fed1-45a7-a0ad-9d8eb4306fc3","metadata":{},"outputs":[],"source":["Now let's create user function that will calculate features importance of defined classificator model. And let's create the variable, that contains features sorted by importance in descending order.\n"]},{"cell_type":"code","id":"a10c6dd4-cbbb-46d7-acd1-e05d067b24ed","metadata":{},"outputs":[],"source":["def model_imp(x, y, model):\n    feat_importances = pd.Series(model.feature_importances_, index=x.columns)\n    return feat_importances.sort_values(ascending=False)\nimp = model_imp(x_enc, y, model)\nprint(imp)"]},{"cell_type":"markdown","id":"5efa8901-0117-42b8-be3c-67144c846ebd","metadata":{},"outputs":[],"source":["Plot graph of feature importances for better visualization\n"]},{"cell_type":"code","id":"6e26f577-7b08-4c02-9e2c-97cc4ecb28a7","metadata":{},"outputs":[],"source":["imp.nlargest(10).plot(kind='barh')\nplt.show()"]},{"cell_type":"markdown","id":"9b747525-6abb-417e-b559-e16b1c73812c","metadata":{},"outputs":[],"source":["Build plot that show accuracy of defined model dependence on numbers of input features.\n"]},{"cell_type":"code","id":"0fca101a-870f-4951-8fa3-f7398fecc63f","metadata":{},"outputs":[],"source":["col = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(x_enc[col], y, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()"]},{"cell_type":"markdown","id":"149be6e9-3004-417b-b9a1-b7f97a597ca8","metadata":{},"outputs":[],"source":["We can see that 5 features is enough to make 100% accuracy. So let's create list of this 5 features in order to use them for our next classification models.\n"]},{"cell_type":"code","id":"c0ed548b-5fad-4c49-82a5-25193aa1a4b5","metadata":{},"outputs":[],"source":["col = imp.nlargest(5).index\ncol"]},{"cell_type":"markdown","id":"4a51893c-4812-4981-b5d7-ca6ba7f35c81","metadata":{},"outputs":[],"source":["Let's refit the model on most important features\n"]},{"cell_type":"code","id":"f38dcef3-2b9c-4067-a10b-ca961ecd3672","metadata":{},"outputs":[],"source":["X_most_imp = x_enc[col]\nmodel.fit(X_most_imp, y)\nyhat = model.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"38f9e7c1-779e-48fb-9e21-5268244257cf","metadata":{},"outputs":[],"source":["### Visualization of decision tree\n"]},{"cell_type":"markdown","id":"e2de172b-3ec9-4c98-8f14-682d1c8e17bf","metadata":{},"outputs":[],"source":["Let's visualize decision tree.\n","There are some ways to do it. \n"]},{"cell_type":"markdown","id":"71b330b9-5386-4ccc-b65f-8da1b71e66ee","metadata":{},"outputs":[],"source":["### _Text visualization_\n"]},{"cell_type":"code","id":"3a734470-b807-414e-9f00-b43edd0b9e40","metadata":{},"outputs":[],"source":["text_representation = tree.export_text(model)\nprint(text_representation)"]},{"cell_type":"markdown","id":"288087b9-e07c-459c-ab8f-da22eb80a1b4","metadata":{},"outputs":[],"source":["You can save it into file:\n"]},{"cell_type":"code","id":"59a482d0-302d-4947-bedb-fc0478635116","metadata":{},"outputs":[],"source":["with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)"]},{"cell_type":"markdown","id":"91c05d48-eeb8-4b20-9e84-d6dbb83f1ea7","metadata":{},"outputs":[],"source":["### _Plot tree_\n"]},{"cell_type":"markdown","id":"d9220880-812e-4c56-a342-c625eb529c4d","metadata":{},"outputs":[],"source":["You can plot tree using by two different way:\n"]},{"cell_type":"markdown","id":"c122ff47-ef5a-4cfa-a85a-ede6932769e8","metadata":{},"outputs":[],"source":["**[plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)** (slow render - this can take some time): \n"]},{"cell_type":"code","id":"c676e349-6e50-4283-8d46-9d00b4e3761f","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model,\n               feature_names = col, \n               filled = True)"]},{"cell_type":"code","id":"d391c1ce-602e-4a87-a4dc-1abfc5824835","metadata":{},"outputs":[],"source":["fig.savefig('decision_tree.png')"]},{"cell_type":"markdown","id":"6e1e1005-8ee7-4820-a6e2-2860322fe42d","metadata":{},"outputs":[],"source":["Or you can use **[python-graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)** library. This is more fast function\n"]},{"cell_type":"code","id":"b579769e-b74e-4fd5-a133-b6b9e5ecd0b9","metadata":{},"outputs":[],"source":["# conda install python-graphviz"]},{"cell_type":"code","id":"38253279-e70c-4736-bca9-e07323cb21df","metadata":{},"outputs":[],"source":["dot_data = tree.export_graphviz(model,\n               feature_names = col,\n                                filled=True)"]},{"cell_type":"markdown","id":"51f6558a-6e04-4668-9605-1faff180c2b5","metadata":{},"outputs":[],"source":["After creation you can draw graph\n"]},{"cell_type":"code","id":"bebb1b82-f88d-4bf9-b700-31705367b1ee","metadata":{},"outputs":[],"source":["graph = graphviz.Source(dot_data, format=\"png\") \ngraph"]},{"cell_type":"markdown","id":"2fd360b2-0ed7-478b-8f17-0b4c11fc2e73","metadata":{},"outputs":[],"source":["And render it into file:\n"]},{"cell_type":"code","id":"4af9e325-0939-4c16-89ef-0ba4674404b5","metadata":{},"outputs":[],"source":["graph.render(\"decision_tree_graphivz\")"]},{"cell_type":"markdown","id":"65087967-7fd4-4ee7-ad71-60ae50e7716c","metadata":{},"outputs":[],"source":["### Extra Trees Classifier\n"]},{"cell_type":"markdown","id":"b6d3a355-e5d9-4782-a50d-0deba7e2609a","metadata":{},"outputs":[],"source":["Let's create and fit ExtraTreesClassifier on most important features and calculate accuracy of classification:\n"]},{"cell_type":"code","id":"58e30094-93b3-4e5c-b73e-084ab9f8f4c3","metadata":{},"outputs":[],"source":["model = ExtraTreesClassifier()\nmodel.fit(X_most_imp, y)"]},{"cell_type":"markdown","id":"92683cd5-eeac-40c7-bb6a-99379d925a64","metadata":{},"outputs":[],"source":["Evaluate the model for obtain predictions\n"]},{"cell_type":"code","id":"395339ea-b11b-492b-928f-0b8e5e2bd58a","metadata":{},"outputs":[],"source":["yhat = model.predict(X_most_imp)\nprint(yhat)"]},{"cell_type":"markdown","id":"46027678-6cb3-493f-b756-80569068073c","metadata":{},"outputs":[],"source":["Evaluate accuracy: \n"]},{"cell_type":"code","id":"38c22c71-8b68-470f-81ee-52cdd12da763","metadata":{},"outputs":[],"source":["accuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"4912a90c-4e61-45d0-abbf-52e38fbaf557","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \u003ch1\u003eQuestion 2:\u003c/h1\u003e\n","    \u003cp\u003eCreate the variable, that contains features sorted by importance in descending order for Extra Tree model (using 5 most important features)\u003c/p\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"21d7aa3b-83a4-431d-948a-b68bd461c194","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n"]},{"cell_type":"markdown","id":"015a841d-7852-42a2-b9c1-c0ef14d9dee9","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","imp = model_imp(X_most_imp, y, model)\n","print(imp)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"81796056-55e2-4d32-8443-2dec7418a6d6","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \u003ch1\u003eQuestion 3:\u003c/h1\u003e\n","    \u003cp\u003eBuild plot that show accuracy of Extra Tree model dependence on numbers of input features (using 5 most important features).\u003c/p\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"2f744afc-e06a-49f2-8505-698f67600535","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n"]},{"cell_type":"markdown","id":"5b8d6e47-db45-47a4-a107-b4e1d6903f59","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","col = []\n","ac = []\n","for c in imp.index:\n","    col.append(c)\n","    ac.append(model_ac(X_most_imp[col], y, model))\n","    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\n","ac = pd.Series(ac)\n","ac.plot()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"a2d5feaf-1abf-4b2b-9121-d46d863f61d4","metadata":{},"outputs":[],"source":["### Logistic regression \n"]},{"cell_type":"markdown","id":"40cfc000-2eeb-418d-8c37-96660eeca308","metadata":{},"outputs":[],"source":["There are many different techniques for scoring features and selecting features based on scores; how do you know which one to use?\n","\n","A robust approach is to evaluate models using different feature selection methods (and numbers of features) and select the method that results in a model with the best performance.\n","\n","**[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0MI1EN2415-2022-01-01)** is a good model for testing feature selection methods as it can perform better if irrelevant features are removed from the model. We will use this model in absolutelly similar way like previous one.\n"]},{"cell_type":"code","id":"8a628691-d941-4ec2-9d06-d96977197122","metadata":{},"outputs":[],"source":["model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_most_imp, y)\nyhat = model.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"6640f6fd-9541-4dac-a554-1012db378b12","metadata":{},"outputs":[],"source":["As we can see, accuracy of Logistic Regression model is lower (about 80%).\n"]},{"cell_type":"markdown","id":"0668c004-8987-4f8f-b923-59b6f0a4f5e9","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \u003ch1\u003eQuestion 4:\u003c/h1\u003e\n","    \u003cp\u003eCalculate accuracy of Logistic Regression model using all features.\u003c/p\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"22c8d0c5-1b5e-4fca-ac39-70db73272264","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n"]},{"cell_type":"markdown","id":"f4a21eea-850d-4c63-a016-feddb8ad7d1d","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","model = LogisticRegression(solver='lbfgs')\n","model.fit(x_enc, y)\n","yhat = model.predict(x_enc)\n","accuracy = accuracy_score(y, yhat)\n","print('Accuracy: %.2f' % (accuracy*100))\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"9eb9a79f-29c2-4a71-8de0-f9424ff8399f","metadata":{},"outputs":[],"source":["## Conclusions\n"]},{"cell_type":"markdown","id":"f82076d8-2dca-496d-a2d5-66f2c4286ce4","metadata":{},"outputs":[],"source":["In this lab we learned to do preliminary data processing. In particular, change data types, normalize and process categorical data. It was shown how to make feature selection by different methods. Shows how to work with different classifiers. It was also shown how to visualize a decision tree.\n","As a result of lab it was shown how on the basis of a statistical database predict if the patient will die of a heart failure.\n","\n","The accuracy of Decision Tree and Extra Tree classifiers was 100%. And the accuracy of Logistic Regression about 80%. \n"]},{"cell_type":"markdown","id":"c950e30f-5601-4445-be01-f3e583b82c0e","metadata":{},"outputs":[],"source":["### Thank you for completing this lab!\n","\n","## Author\n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e\n","\n","### Other Contributors\n","\n","\u003ca href=\"https://www.linkedin.com/in/mahdi-noorian-58219234/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\"\u003eMahdi Noorian PhD\u003c/a\u003e\n","\n","Bahare Talayian\n","\n","Eric Xiao\n","\n","Steven Dong\n","\n","Parizad\n","\n","Hima Vasudevan\n","\n","\u003ca href=\"https://www.linkedin.com/in/fiorellawever/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\"\u003eFiorella Wenver\u003c/a\u003e\n","\n","\u003ca href=\"https:// https://www.linkedin.com/in/yi-leng-yao-84451275/ \" target=\"_blank\" \u003eYi Yao\u003c/a\u003e.\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                 |\n","| ----------------- | ------- | ---------- | ---------------------------------- |\n","\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}