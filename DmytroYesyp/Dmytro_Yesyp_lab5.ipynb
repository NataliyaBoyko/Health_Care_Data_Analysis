{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc97f92a-85e1-43c1-886d-93968bdb1f25",
      "metadata": {},
      "outputs": [],
      "source": [
        "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode_vertical.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97889390-9395-4567-8c9d-8a59ba81c973",
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Investigation of diabetes patients readmission among US hospitals**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f85748-4214-4d59-bf39-e4def49234de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab 5 Data Analysis with Python\n\nEstimated time needed: **45** minutes\n\nThis lab is dedicated to the study of machine learning classification methods. The goal is to predict whether the patient will be readmitted or not.\n\n## Objectives\n\n* Download DataSet from * .csv files\n* Conduct basic data analysis\n* Calculate new and change column types\n* Divide the DataSet into training and test\n* Use different machine learning classification methods\n* Combine classifiers into ensemble\n* Calculate accuracy and analyze errors\n* Visualize the decision tree\n* Combine all stages of data analysis with Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab815424-8784-4ad8-8d67-06960130d2c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b896e674-816c-41d9-9a05-96090cc208b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li>Materials and methods\n            <ul>\n                <li>Prerequisites</li>\n            </ul>\n        </li>\n        <li>Import Libraries</li>\n        <li>Load the Dataset</li>\n        <li>Data pre-preparation</li>\n        <li>Pipiline Classification\n             <ul>\n                <li>RandomForestClassifier</li>\n                 <li>Cross-validation</li>\n                 <li>Accuracy</li>\n            </ul>\n        </li>\n         <li>Over-sampling proble</li>\n        <li>Ensemble of classifiers\n            <ul>\n                <li>Question 1</li>\n            </ul>\n        </li>\n        <li>Conclusions</li>\n        <li>Authors</li>\n    </ol>\n</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c97125d-3f9c-4403-abe7-f407b90c4182",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Materials and methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2985fbf9-97cc-4c19-a28f-6990b0ea638c",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab, we will learn how to download and pre-prepare data, classify and combine classifiers into an ensemble.\nThis lab consists of the following steps:\n* Download data - download and display data from a file\n* Preliminary data preparation - preliminary analysis of data structure, change of data structure and tables\n* Pipeline classification - classification and analysis by grouping stages\n    * Logistic regression - classification and analysis of accuracy and errors using logistic regression\n    * Over-sampling problem - solve the problem of uneven distribution of data\n    * Ensemble of classifiers - study various classifiers and methods of combining them into an ensemble\n    * Decision tree - shows how to visualize the decision tree and determine the importance of factors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6837f1-8656-4340-bf67-ab7b3e112132",
      "metadata": {},
      "outputs": [],
      "source": [
        "The data that we are going to use for this is a subset of an open source diabetes in US DataSet: https://www.kaggle.com/datasets/brandao/diabetes.\n> This dataset is public available for research.\nPlease include this citation if you plan to use this database:\nThe DataSet represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a94c4bc-ac99-40ff-a097-d0a1ff17c3b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Prerequisites\n* [Python](https://www.python.org) - middle level\n* [Pandas](https://pandas.pydata.org) - middle level \n* [Matplotlib](https://matplotlib.org) - basic level\n* [SeaBorn](https://seaborn.pydata.org) - basic level\n* [Scikit-Learn](https://scikit-learn.org/stable/) - middle level "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9894767-361a-4613-8e12-e84452302881",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import Libraries/Define Auxiliary Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719a33b9-54ec-4708-a890-173e99d6bce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Running outside Skills Network Labs.** This notebook was tested within Skills Network Labs. Running in another environment should work as well, but is not guaranteed and may require different setup routine."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77be4612-455c-42ae-aa38-1d0626a6044f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Libraries such as Pandas, MatplotLib, SeaBorn, Scikit-Learn, imbalanced-learn, python-graphviz should be installed."
      ]
    },
    {
      "cell_type": "code",
      "id": "aa97ebd1-dbfb-4e37-ae4a-88a4d7b292f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge pandas"
      ]
    },
    {
      "cell_type": "code",
      "id": "849a9ebd-b43a-4bb6-8291-8e351cfb675f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "id": "af8afbc6-48dc-47f7-aa4f-adc484e1cb06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge seaborn "
      ]
    },
    {
      "cell_type": "code",
      "id": "125d767d-f887-46fc-ae92-76c28e39ede4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c intel scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "id": "cce78732-3b88-43c8-8f46-a83576ace5c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "id": "06595ca3-ed38-4faf-9ba5-0bbb04fee2bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install python-graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "613e8d83-479a-4caf-8b5d-7753abfb4473",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Download data from a .csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb1f0b1-0fce-4340-8cd1-863f3516e372",
      "metadata": {},
      "outputs": [],
      "source": [
        "Some libraries should be imported before you can begin."
      ]
    },
    {
      "cell_type": "code",
      "id": "7bbe14ff-9f2e-4569-a856-d6a5a176dabb",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "id": "aee0ee42-39e3-45e8-b925-833a70916b0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler                         \nfrom sklearn.compose import make_column_transformer\nfrom sklearn import set_config\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import recall_score, precision_score\nimport time\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4adbc8-a754-4289-baf8-a6a4787c7cbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's disable warnings by **[warnings.filterwarnings()](https://docs.python.org/3/library/warnings.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "3590957b-c138-4029-b32d-8eaf0ecd26f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17720cca-55a1-4cc8-a810-615e346a80d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "The next step is to download the data file from the repository by **[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)**.\n\nWe will use the same DataSet like in previous lab. Therefore next some steps will be the same."
      ]
    },
    {
      "cell_type": "code",
      "id": "3950a20a-407b-4a40-8f66-656b56aef328",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05SZEN/clean_df.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2772c65d-4361-467d-99e9-ef9c1bf759d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now let's look at our DataSet."
      ]
    },
    {
      "cell_type": "code",
      "id": "6cf1dbfa-674e-4b6d-b2c4-453a33a1ef4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89562ce2-122c-4f31-85cb-78a4164f31a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Data pre-preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596aa86b-6b75-4399-833a-2485cf12f4f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's study DataSet. As you can see DataSet consist 101388 rows × 55 columns. As you can see DataSet consist information of different types. We should be sure that python recognized data types in right way. To do this we shoul use **[pandas.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)**."
      ]
    },
    {
      "cell_type": "code",
      "id": "ab85b1de-f1a5-41c8-b23b-4dc57cef209f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f947fb17-7b17-4199-922a-ff9e19679acf",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details>\n<summary><b>Click to see attribute information</b></summary>\n\n1. `Encounter Id` - Unique identifier of an encounter  (int64)\n2. `Patient Number` - Unique identifier of a patient (int64)\n3. `Race` - (categorical: `Caucasian` `AfricanAmerican` `Other` `Asian` `Hispanic`)\n4. `Gender` - (categorical: `Female` `Male` `Unknown/Invalid`)\n5. `Age` -  Grouped in 10-year intervals (categorical: `[0-10)` `[10-20)` `[20-30)` `[30-40)` `[40-50)` `[50-60)` `[60-70)` `[70-80)` `[80-90)` `[90-100)`)\n6. `Weight` -  Weight in pounds (categorical: `[75-100)` `[50-75)` `[0-25)` `[100-125)` `[25-50)` `[125-150)` `[175-200)` `[150-175)` `>200`)\n7. `Admission Type Id` - Integer identifier corresponding to 9 distinct values, for example, emergency, urgent, elective, newborn, and not available (int64)\n8. `Discharge Disposition Id` - Integer identifier corresponding to 29 distinct values, for example, discharged to home, expired, and not available (int64)\n9. `Admission Source Id` - Integer identifier corresponding to 21 distinct values, for example, physician referral, emergency room, and transfer from a hospital (int64)\n10. `Time In Hospital` - Integer number of days between admission and discharge (int64)\n11. `Payer Code` - Integer identifier corresponding to 23 distinct values, for example, Blue Cross\\Blue Shield, Medicare, and self-pay (categorical)\n12. `Medical Specialty` - Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family\\general practice, and surgeon (categorical)\n13. `Num Lab Procedures` - Number of lab tests performed during the encounter (float64)\n14. `Num Procedures` -  Number of procedures (other than lab tests) performed during the encounter (int64)\n15. `Num Medications` - Number of distinct generic names administered during the encounter (int64)\n16. `Number Outpatient` - Number of outpatient visits of the patient in the year preceding the encounter (int64)\n17. `Number Emergency` - Number of emergency visits of the patient in the year preceding the encounter (int64)\n18. `Number Inpatient` - Number of inpatient visits of the patient in the year preceding the encounter(int64)\n19. `Diagnosis1` - The primary diagnosis (coded as first three digits of ICD9) (categorical)\n20. `Diagnosis2` - Secondary diagnosis (coded as first three digits of ICD9) (categorical)\n21. `Diagnosis3` - Additional secondary diagnosis (coded as first three digits of ICD9) (categorical)\n22. `Number Diagnoses` - Number of diagnoses entered to the system (float64)\n23. `Max Glu Serum` - Indicates the range of the result or if the test was not taken. Values: `>200`, `>300`, `normal`, and `none` if not measured (categorical)\n24. `A1c Result` - Indicates the range of the result or if the test was not taken. Values: `>8` if the result was greater than 8%, `>7` if the result was greater than 7% but less than 8%, `normal` if the result was less than 7%, and “none” if not measured (categorical)\n25. `Metformin` - patient medications (categorical)\n26. `Repaglinide` - patient medications (categorical)\n27. `Nateglinide` - patient medications (categorical)\n28. `Chlorpropamide` - patient medications (categorical)\n29. `Glimepiride` - patient medications (categorical)\n30. `Acetohexamide` - patient medications (categorical)\n31. `Glipizide` - patient medications (categorical)\n32. `Glyburide` - patient medications (categorical)\n33. `Tolbutamide` - patient medications (categorical)\n34. `Pioglitazone` - patient medications (categorical)\n35. `Acarbose` - patient medications (categorical)\n36. `Miglitol` - patient medications (categorical)\n37. `Troglitazone` - patient medications (categorical)\n38. `Tolazamide` - patient medications (categorical)\n39. `Examide` - patient medications (categorical)\n40. `Citoglipton` - patient medications (categorical)\n41. `Insulin` - patient medications (categorical)\n42. `Glyburide-metformin` - patient medications (categorical)\n43. `Glipizide-metformin` - patient medications (categorical)\n44. `Glimepiride-pioglitazone` - patient medications (categorical)\n45. `Metformin-rosiglitazone` - patient medications (categorical)\n46. `Metformin-pioglitazone` - patient medications (categorical)\n47. `Diabetes Medication` -  Indicates if there was any diabetic medication prescribed. Values: `True` and `False` (bool)\n48. `Readmitted` [Target Column] - Days to inpatient readmission. Values: `<30` if the patient was readmitted in less than 30 days, `>30` if the patient was readmitted in more than 30 days, and `No` for no record of readmission (categorical)\n49. `ages-binned`(categorical)\n50. `change_yes` - columns created in previous labs (int64)\n51. `change_no` - columns created in previous labs (int64)\n52. `Increased` - columns created in previous labs (int64)\n53. `No` - columns created in previous labs (int64)\n54. `Steady` - columns created in previous labs (int64)\n55. `Decreased` - columns created in previous labs (int64)\n    \n    </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a048d3-b3b6-44dd-9c79-0cdf2430fe19",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's study information of DataSet columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2585a73d-4b2c-4446-8631-e330f9a0dc26",
      "metadata": {},
      "outputs": [],
      "source": [
        "Here we have a lot of columns that have a limited set of values and their type is \"object\", so for correct analysis, change their type to categorical."
      ]
    },
    {
      "cell_type": "code",
      "id": "973a5620-d3c5-4e87-8f89-59d5d36955ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "obj_cols = df.select_dtypes(include='object').columns\n\ndf[obj_cols] = df[obj_cols].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47777227-1651-482f-8262-582fc5cd5293",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now let's delete columns that have no impact on our model, as we did in the previous lab.(**[pandas.DataFrame.drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)**)."
      ]
    },
    {
      "cell_type": "code",
      "id": "40fe60d8-125a-4a27-8511-771e81267c8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(['Encounter Id', 'Patient Number', 'Payer Code','Examide','Citoglipton'], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e696fa-eb37-433d-aae3-a4435c681395",
      "metadata": {},
      "outputs": [],
      "source": [
        "The resulting dataset will be sized (**[pandas.DataFrame.shape](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html?highlight=shape#pandas.DataFrame.shape)**):"
      ]
    },
    {
      "cell_type": "code",
      "id": "db984d91-7a2e-4fd3-b38e-e1c9ac27ef1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "id": "db2b2aa4-fc17-4d73-9bb4-d598896acd43",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e4043dd-26ee-43c5-8c48-bfd05fb1ecb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Pipiline Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c19580-c461-4b11-a3e2-2ec7f31055dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "### RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb71dfde-03b1-47c1-92d2-df9b77c09d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "Before classification, the dataset must be divided into input and target factors."
      ]
    },
    {
      "cell_type": "code",
      "id": "731786d8-8654-4e77-9ce2-07e46ebb6787",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = df.drop(columns = ['Readmitted'])"
      ]
    },
    {
      "cell_type": "code",
      "id": "19b47074-ccdb-4515-a568-f392371ba87a",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df['Readmitted']"
      ]
    },
    {
      "cell_type": "code",
      "id": "dc4e9ebd-c02b-463e-9724-6580b1f55f1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85ebd66-5cf0-49a3-af2d-9cefb58d5f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can see the input data set consists from 49 columns.\nAs you can see, 31 columns are categorical, 15 - numerical, 1 boolean and 2 float. To make classification, all numerical, boolean and float fields must be normalized and categorical fields must be digitized. This can be automated using the **[sklearn.preprocessing.OrdinalEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** and **[sklearn. preprocessing.StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)**.\n\nSince the machine learning process consists of several steps, each of which has the function `fit`,` predict` and etc, we can combine all these stages into one block using `Pipeline` (**[sklearn.pipeline.make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)**), **[sklearn.compose.make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)** and visualize it with: **[sklearn.set_config()](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493edd2b-1c73-4623-89b2-fcfbeb017d1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Select all categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "id": "b58ef196-fa37-40d5-b746-e46a9bb68b0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_col = x.select_dtypes(include=['category']).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db0766e-754f-42d6-a564-e685f9dbeec9",
      "metadata": {},
      "outputs": [],
      "source": [
        "Here select all numerical, boolean and float columns"
      ]
    },
    {
      "cell_type": "code",
      "id": "c91482ea-a392-440b-9e16-e2f9c24015f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_col = x.select_dtypes(include=['int64','float','boolean']).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3607fd31-5e66-4d61-b093-98b316e81502",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now create transformer for our previously selected columns."
      ]
    },
    {
      "cell_type": "code",
      "id": "e15e6d63-93b4-40c6-b103-5af59dedf775",
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = make_column_transformer((OrdinalEncoder(handle_unknown = 'use_encoded_value',unknown_value = -1),cat_col),\n                                (StandardScaler(),numeric_col),\n                                remainder = 'passthrough')\nset_config(display = 'diagram')\ntrans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f30ea1-781b-40fa-916c-6433ef80bcd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "Next we must separate DataSets for train and test DataSets for calculate accuracy of models. To do this we can use **[sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)**. Let's separate DataSets in 0.33 proportion train/test"
      ]
    },
    {
      "cell_type": "code",
      "id": "95d4f207-91e1-4a6d-ba6a-392a26ca48cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.33, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "id": "17e30b01-74ff-42b3-8bfc-883ecd177e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "id": "429a332a-e8aa-425b-818b-2cb679d9b8bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cb1297-a5bf-4def-a37d-a99846851898",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now let's create a RandomForestClassifier model (**[sklearn.linear_model.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**) and add it to our `Pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "id": "1ff418e6-7f00-42cb-bb02-cadc4f1cdc15",
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier()\npipe_rfc = make_pipeline(trans,rfc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b350e91b-0135-401a-9087-ae97a8f2ada3",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's fit our model and calculate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "id": "52eca1a5-99e5-4798-a7a7-b03a3ded4338",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_rfc.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c1c8938-c042-48e4-b83c-7bedb3cb1560",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82c6ca8-4f2b-4d4f-b3f3-55f9878d945d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Cross-validation is a technique in machine learning where the available DataSet is split into multiple subsets or folds, and the model is trained and tested on different subsets in a rotation. The primary purpose of cross-validation is to estimate how well the model is expected to perform when it is deployed to make predictions on new, unseen data.\n\nOne common way to implement cross-validation is by using the cross_val_score helper function, which takes an estimator (the model to be trained and tested) and the DataSet, and returns the scores from each fold. This allows for easy evaluation and comparison of different models based on their performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "id": "84d6217d-b857-493a-9860-83749eb4b0c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "Rcross = cross_val_score(pipe_rfc,x ,y, cv = 4)\nprint(Rcross)\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standart deviation is\", Rcross.std())"
      ]
    },
    {
      "cell_type": "code",
      "id": "4b3af3ab-2fdf-4065-8b7c-99765520e8fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = cross_val_predict(pipe_rfc,x,y,cv = 4)\nyhat[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b36e7b-a359-4970-88c4-61d71cd70180",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "id": "f5945240-54f9-4fdd-a7ec-0a41630efc4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = pipe_rfc.score(x_train, y_train)\nscores_test = pipe_rfc.score(x_test, y_test)\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291befc4-ee0f-49b6-a8bb-96a493d3527f",
      "metadata": {},
      "outputs": [],
      "source": [
        " As we use a random forest classifier, accuracy can change a little, so to get a better result, you can restart an upper block of code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d26729-a21d-4285-9eb2-ba0b81091e3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's evaluate the correctness of the classification with: **[sklearn.metrics.plot_confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)** and convince of these conclusions."
      ]
    },
    {
      "cell_type": "code",
      "id": "1e87a525-b84c-47a2-8141-16a86722a2a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_estimator(pipe_rfc, x_test, y_test,cmap=plt.cm.Blues)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2e1205-8aae-4483-b94e-169134aab445",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, for test accuracy, we get ~57%, which for medical data is a good result.\n\nThe `Recall` metric is used to assess the accuracy of only purchased goods: **[sklearn.metrics.recall_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "404cf742-0c8d-469b-9c97-fb975c979b4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = recall_score(y_train, pipe_rfc.predict(x_train), average='micro')\nscores_test = recall_score(y_test, pipe_rfc.predict(x_test), average='micro')\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a479a52-a721-420e-9dc0-4f8c06ee15bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "As can be seen from this metric, the accuracy is very low. Moreover, the accuracy of the training and test data are approximately the same. This means that in order to increase this metric of accuracy, it is necessary to increase the training sample. Let's analyze it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adf29020-847c-4003-9e0f-360732a3f944",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Over-sampling problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca0b0ab-808f-41d2-8d9a-0cd3c8d103aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's analyze the number of rejections and accepted offers to purchase goods (**[seaborn.countplot()](https://seaborn.pydata.org/generated/seaborn.countplot.html)**):"
      ]
    },
    {
      "cell_type": "code",
      "id": "0eff6895-a4d9-4ed2-98d1-c801707a9014",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.countplot(x = y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20a99e0-599c-443b-977a-36d165b30db1",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, the number of rejections is much greater than the number of accepted proposals. To balance the data set, we can use a special function: **[imblearn.over_sampling.RandomOverSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)**:"
      ]
    },
    {
      "cell_type": "code",
      "id": "177f108e-7e8a-4144-92af-22753369b919",
      "metadata": {},
      "outputs": [],
      "source": [
        "ROS = RandomOverSampler()\npipe_ros = make_pipeline(trans,ROS)\no_x, o_y = pipe_ros.fit_resample(x_test,y_test)\nsns.countplot(x = o_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d65556b-302a-411f-b1e8-4d76e38c1af7",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's add this function to our `Pipeline`, fit the model and recalculate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "id": "fe5459f9-50f4-41e0-8498-a9261530fa2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_rfc = make_pipeline(trans, ROS, rfc)\npipe_s_rfc"
      ]
    },
    {
      "cell_type": "code",
      "id": "2e691ba2-b67c-40d1-b9f4-af1a5539e728",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_rfc.fit(x_train,y_train)\nscores_train = recall_score(y_train, pipe_s_rfc.predict(x_train), average = 'weighted')\nscores_test = recall_score(y_test, pipe_s_rfc.predict(x_test), average = 'weighted')\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff364a4-fbd9-429f-939a-dc258a6eefe2",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, balancing the dataset has led to a sharp decrease in the accuracy of the `Recall` metric.\n\nLet's analyze the errors of the model."
      ]
    },
    {
      "cell_type": "code",
      "id": "bc761167-2e28-4c87-b6ef-1e7cf3189fba",
      "metadata": {},
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_estimator(pipe_s_rfc, x_test, y_test,cmap=plt.cm.Blues)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f17e1c-ae0f-4480-b888-c77ba7f7e644",
      "metadata": {},
      "outputs": [],
      "source": [
        "As can be seen, the number of erroneous predictions slightly increased. However, the error is high when the model predicts a positive decision, and in fact the customer refuses to buy. The metric `Precision` is used to assess this accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e692ec-ede5-47eb-ac10-2be05d42d3cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Ensemble of classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d93264-7554-4e3f-85bf-acd80912d151",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's test other classifiers and compare the results.\nWe will test:\n\n* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n* [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n* [Ada Boost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n* [Etra Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n* [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25dd50e-3e47-4922-9b6f-f8c71d4d79c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "In addition, different classifiers may err in different situations. Therefore, to compensate for each other's mistakes, it is necessary to use model ensembles by Voting Classifier.\n\nA **[Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)** is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\nIt simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n\nVoting Classifier supports two types of votings.\n\n**Hard Voting**: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n\n\n**Soft Voting**: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "4d6b6c83-8da1-4a80-a15c-9736213d3f1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_s = make_pipeline(trans, ROS)\nnames = [\"Logistic Regression\",\n         \"Decision Tree\", \"Random Forest\",\"Gaussian Naive Bayes\"]\nclassifiers = [\n    LogisticRegression(),\n    DecisionTreeClassifier(max_depth=8),\n    RandomForestClassifier(n_estimators=10, max_features=1),\n    GaussianNB(),\n    ]\n\nscores_train = []\nscores_test = []\nscores_train_s = []\nscores_test_s = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57109639-95ef-4a01-bbf4-7fb75bf8eb78",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can use other classifiers such as Extra tree classifier, Gradient Boosting Classifier ,etc. But it takes significantly more time to calculate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36386858-b62d-447f-beb1-b0d6830b26e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  1: </h1>\n\n<b>Try out other classification models.</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "6e13c60b-ddf3-4ef3-983b-f5fa017b4857",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057dc3a4-c50a-4545-b01b-0c4527162225",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click <b>here</b> for the solution</summary>\n    \n```python\n\nnames += [\"Ada Boost Classifier\",\"Etra Tree Classifier\",\"Gradient Boosting Classifier\"]\nclassifiers += [\n    AdaBoostClassifier(),\n    ExtraTreesClassifier(),\n    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n    ]\n```\n    \n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84db2bd9-5e01-4ef3-9c7d-6fcb26c4b172",
      "metadata": {},
      "outputs": [],
      "source": [
        "Run all classifiers."
      ]
    },
    {
      "cell_type": "code",
      "id": "b9596505-09d1-4886-847f-1de92983f0bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "est = [(str(est), est) for est in classifiers]\neclf = [VotingClassifier(\n     estimators=est,\n     voting='hard')]\nnames += [\"Voting Classifier\"]\nclassifiers += eclf\nfor name, classif in zip(names, classifiers):\n    start_time = time.time()\n    print(name,'fitting.....',end = '')\n    clf = make_pipeline(trans, classif)\n    clf.fit(x_train,y_train)\n    score_train = recall_score(y_train, clf.predict(x_train), average='micro')\n    score_test = recall_score(y_test, clf.predict(x_test), average='micro')\n    scores_train.append(score_train)\n    scores_test.append(score_test)\n    \n    clf_s = make_pipeline(trans, ROS, classif)\n    clf_s.fit(x_train,y_train)\n    score_train_s = recall_score(y_train, clf_s.predict(x_train), average='micro')\n    score_test_s = recall_score(y_test, clf_s.predict(x_test), average='micro')\n    scores_train_s.append(score_train_s)\n    scores_test_s.append(score_test_s)\n    end_time = time.time()\n    print(\" [\",round(end_time - start_time,2),\"s]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ea23e8-7db3-4bd2-92a2-92b4c1fc20fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's compare the accuracy of classifiers for balanced and unbalanced data sets."
      ]
    },
    {
      "cell_type": "code",
      "id": "0c1eca94-c8fd-4532-800a-b3085190d22f",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = pd.DataFrame(index = names)\nres['Train'] = np.array(scores_train)\nres['Test'] = np.array(scores_test)\nres['Train Over Sampler'] = np.array(scores_train_s)\nres['Test Over Sampler'] = np.array(scores_test_s)\n\nres.index.name = \"Classifier accuracy\"\npd.options.display.float_format = '{:,.2f}'.format\nres"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5a2f16-32a0-422a-b596-a9fccf30524c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Diagram representation of table above."
      ]
    },
    {
      "cell_type": "code",
      "id": "d723f2ef-6506-4f91-800e-59c341c19e27",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,6))\nax.bar(names, scores_test)\nax.bar(names, scores_test_s)\nax.legend(['Test', 'Test Over Sampler'])\n\nax.set_title('Calassifiers Accuracy')\nax.set_xlabel('Classifier')\nax.set_ylabel('Accuracy')\n\nplt.xticks(rotation=45)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f30248-b7a2-4813-a441-1e042b63448b",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, the balanced data set leads to a sharp increase in accuracy in all classifiers. It can also be seen that the most accurate model was logistic regression. The ensemble of models showed better accuracy on the training data set and slightly worse on the test.\n\nLet's displey the last classifier:"
      ]
    },
    {
      "cell_type": "code",
      "id": "51d9a2f1-18f4-49ba-861c-602fdbe472f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c462dc-c1eb-4a6a-b3b5-5a3b590d0b11",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f12ab-13ae-40d3-889f-686e179f2f93",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab we studied how to normalize numerical and categorical data. It was shown how to build training and test data sets. Shows how to fit different classifiers, evaluate their accuracy and analyze errors.\nWe also studied how to join them together in an ensemble and create a model based on Pipeline.\nWe compared the accuracy of different classifiers and their ensemble and showed how they can be used in diabetes patients prediction.\n\nThe accuracy of the decision was about 60%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92d9ae8-55e5-40ce-a245-e3f43a147e9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Author"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9278a078-e4f0-47d3-bea9-e68a36e004a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "Developer: [Yaroslav Vyklyuk, prof., PhD., DrSc](http://vyklyuk.bukuniver.edu.ua/en/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6653bf23-397d-4737-9e10-2038f041e452",
      "metadata": {},
      "outputs": [],
      "source": [
        " Copyright &copy; 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/)."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}